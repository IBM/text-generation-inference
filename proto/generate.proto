syntax = "proto3";

package generate.v1;

service TextGenerationService {
    /// Service discovery
    rpc ServiceDiscovery (ServiceDiscoveryRequest) returns (ServiceDiscoveryResponse) {}
    /// Empties batch cache
    rpc ClearCache (ClearCacheRequest) returns (ClearCacheResponse);
    /// Empties batch cache
    rpc ModelInfo (ModelInfoRequest) returns (ModelInfoResponse);
    /// Prefill batch and generate first token
    rpc Prefill (PrefillRequest) returns (PrefillResponse);
    /// Generate next token for a list of prefilled batches
    rpc NextToken (NextTokenRequest) returns (NextTokenResponse);
    /// Prune batch
    rpc PruneBatch (PruneBatchRequest) returns (PruneBatchResponse);
    /// Lookup prompt prefix
    rpc PrefixLookup (PrefixLookupRequest) returns (PrefixLookupResponse);
    /// Health check
    rpc Health (HealthRequest) returns (HealthResponse);
}

message HealthRequest {}
message HealthResponse {}

/// Empty request
message ServiceDiscoveryRequest {}

message ServiceDiscoveryResponse {
    /// Other shards urls
    repeated string urls = 1;
}

/// Empty request
message ClearCacheRequest {}

/// Empty response
message ClearCacheResponse {}

/// Empty request
message ModelInfoRequest {}

message ModelInfoResponse {
    enum ModelType {
        CAUSAL_LM = 0;
        SEQ2SEQ_LM = 1;
    }

    ModelType model_type = 1;
    uint32 eos_token = 2;
    /// Whether batches are rectangular/padded (false for flash attention)
    bool batch_padding = 3;
}

message NextTokenChooserParameters {
    message LengthPenalty {
        uint32 start_index = 1;
        float decay_factor = 2;
    }

    /// exponential scaling output probability distribution
    float temperature = 1;
    /// restricting to the k highest probability elements
    uint32 top_k = 2;
    /// restricting to top tokens summing to prob_cut_off <= top_p
    float top_p = 3;
    // the typical probability of a token
    float typical_p = 4;

    /// Minimum tokens to generate
    uint32 min_new_tokens = 100;
    /// optional random seed for sampling
    optional uint64 seed = 101;
    /// optional repetition penalty
    optional float repetition_penalty = 102;
    // optional decay length penalty
    optional LengthPenalty length_penalty = 103;
}

message RequestedDetails {
    bool input_toks = 1;
    bool logprobs = 2;
    bool ranks = 3;
    uint32 top_n_toks = 4;
}

message Request {
    /// Request ID
    uint64 id = 1;
    /// Optional prefix ID
    string prefix_id = 2;
    /// The generation context
    string inputs = 3;
    /// The number of tokens inside inputs
    /// May be fewer if truncation was requested
    uint32 input_length = 4;
    /// Whether input should be truncated (to input_length tokens)
    bool truncate = 5;
    /// Value of request's max_new_tokens
    uint32 max_output_length = 6;
    /// Next Token Chooser Parameters
    NextTokenChooserParameters parameters = 7;

    /// Whether to return tokens individually
    bool stream_response = 100;
    /// Additional details to include in response
    RequestedDetails details = 101;
}

message StopSequence {
    repeated uint32 tokens = 1;
}

message Batch {
    /// Batch ID
    uint64 id = 1;
    /// Individual requests
    repeated Request requests = 2;
    /// Total input tokens in this batch including padding
    uint32 total_tokens = 3;
}

message TopToken {
    uint32 token_id = 1;
    float logprob = 2;
}

message Token {
    /// Request id - not included here for input tokens
    uint64 request_id = 1;
    uint32 token_id = 2;

    // Optional - zero if unset
    float logprob = 3;
    uint32 rank = 4;
    repeated TopToken top_tokens = 5;
}

message GenerateError {
    uint64 request_id = 1;
    string message = 2;
}

message InputTokens {
    uint64 request_id = 1;
    repeated Token tokens = 2;
}

message PrefillRequest {
    /// Batch
    Batch batch = 1;
    /// Optional existing batches with completed requests to be pruned
    repeated CachedBatch to_prune = 2;
}

message GenerateResult {
    /// Next tokens
    repeated Token output_tokens = 1;
    /// Request-specific errors
    repeated GenerateError errors = 2;
    uint64 batch_id = 3;

    /// Time taken by model forward pass in nanoseconds
    uint64 forward_time_ns = 4;
}

message PrefillResponse {
    GenerateResult result = 1;
    
    repeated InputTokens input_tokens = 2; // optional
}

message RequestsStatus {
    /// Ids of finished requests, if any
    repeated uint64 completed_ids = 3;
}

message CachedBatch {
    uint64 batch_id = 1;
    /// If absent, batch is finished
    optional RequestsStatus status = 2;
}

message NextTokenRequest {
    /// Cached batches
    repeated CachedBatch batches = 1;
}


message NextTokenResponse {
    /// Won't be set if batch is completed
    optional GenerateResult result = 1;
}

message PruneBatchRequest {
    /// Batch
    CachedBatch batch = 1;
}

message PruneBatchResponse {
    /// Won't be set if batch is completed
    optional uint64 batch_id = 1;
}

/// Empty request
message PrefixLookupRequest {
    string prefix_id = 1;
}

/// Empty response
message PrefixLookupResponse {
    uint32 prefix_length = 1;
}